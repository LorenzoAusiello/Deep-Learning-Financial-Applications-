{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aware-spouse",
   "metadata": {},
   "source": [
    "Name: Lorenzo Ausiello\n",
    "\n",
    "Date: 02/19/2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fundamental-defense",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fdimino\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import yfinance\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Set seed of random number generator\n",
    "CWID = 20021869 \n",
    "personal = CWID % 10000\n",
    "np.random.seed(personal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handed-rebound",
   "metadata": {},
   "source": [
    "## Question 1 (5pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pediatric-shoulder",
   "metadata": {},
   "source": [
    "### Question 1.1\n",
    "Use the `yfinance` package (or other method of your choice) to obtain the daily adjusted close prices for `SPY` and `IEF`.  You should have at least 5 years of data for both assets. Do **not** include any data after January 1, 2023.  You should inspect the dates for your data to make sure you are including everything appropriately.  Create a binary variable whether the `SPY` returns are above the `IEF` returns on a each day. Create a data frame (or array) of the daily log returns both both stocks along with the lagged returns (at least 2 lags) and your binary class variable.  Use the `print` command to display your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ae435c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  2 of 2 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 SPY       IEF  SPY_lag1  IEF_lag1  SPY_lag2  IEF_lag2  \\\n",
      "Date                                                                     \n",
      "2017-01-09 -0.003306  0.003799  0.003571 -0.004558 -0.000795  0.006462   \n",
      "2017-01-10  0.000000 -0.000474 -0.003306  0.003799  0.003571 -0.004558   \n",
      "2017-01-11  0.002822  0.001137  0.000000 -0.000474 -0.003306  0.003799   \n",
      "2017-01-12 -0.002513  0.000568  0.002822  0.001137  0.000000 -0.000474   \n",
      "2017-01-13  0.002293 -0.002180 -0.002513  0.000568  0.002822  0.001137   \n",
      "...              ...       ...       ...       ...       ...       ...   \n",
      "2022-12-23  0.005736 -0.004538 -0.014369 -0.000308  0.014842  0.001235   \n",
      "2022-12-27 -0.003951 -0.008407  0.005736 -0.004538 -0.014369 -0.000308   \n",
      "2022-12-28 -0.012506 -0.002400 -0.003951 -0.008407  0.005736 -0.004538   \n",
      "2022-12-29  0.017840  0.004899 -0.012506 -0.002400 -0.003951 -0.008407   \n",
      "2022-12-30 -0.002638 -0.004168  0.017840  0.004899 -0.012506 -0.002400   \n",
      "\n",
      "            SPY_lag3  IEF_lag3  SPY>IEF  \n",
      "Date                                     \n",
      "2017-01-09  0.005932  0.001145        0  \n",
      "2017-01-10 -0.000795  0.006462        1  \n",
      "2017-01-11  0.003571 -0.004558        1  \n",
      "2017-01-12 -0.003306  0.003799        0  \n",
      "2017-01-13  0.000000 -0.000474        1  \n",
      "...              ...       ...      ...  \n",
      "2022-12-23  0.001368 -0.007286        1  \n",
      "2022-12-27  0.014842  0.001235        1  \n",
      "2022-12-28 -0.014369 -0.000308        0  \n",
      "2022-12-29  0.005736 -0.004538        1  \n",
      "2022-12-30 -0.003951 -0.008407        1  \n",
      "\n",
      "[1506 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Download historical data for SPY and IEF\n",
    "start_date = datetime(2017, 1, 1)\n",
    "end_date = datetime(2023, 1, 1)\n",
    "\n",
    "myData = yf.download(['SPY', 'IEF'], start=start_date, end=end_date)\n",
    "SPY = myData['Adj Close']['SPY']\n",
    "IEF = myData['Adj Close']['IEF']\n",
    "\n",
    "# Calculate daily log returns\n",
    "rSPY = (np.log(SPY) - np.log(SPY.shift(1))).dropna()\n",
    "rIEF = (np.log(IEF) - np.log(IEF.shift(1))).dropna()\n",
    "\n",
    "df = pd.DataFrame({'SPY':rSPY, 'IEF': rIEF})\n",
    "\n",
    "for i in range(1, 4):\n",
    "    df[f\"SPY_lag{i}\"] = df[\"SPY\"].shift(i)\n",
    "    df[f\"IEF_lag{i}\"] = df[\"IEF\"].shift(i)\n",
    "    \n",
    "df= df.dropna()\n",
    "\n",
    "df['SPY>IEF'] = (df['SPY'] > df['IEF']).astype(int)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "progressive-range",
   "metadata": {},
   "source": [
    "### Question 1.2\n",
    "Split your data into training and testing sets (80% training and 20% test). This split should be done so that the causal relationship is kept consistent (i.e., split data at a specific time).\n",
    "\n",
    "Run a logistic regression of the binary variable (of `SPY` returns greater than `IEF` returns) as a function of the lagged returns (at least 2 lags) for both stocks.\n",
    "This should be of the form (assuming 2 lags) of $p_{t} = [1 + \\exp(-[\\beta_0 + \\beta_{SPY,1} r_{SPY,t-1} + \\beta_{SPY,2} r_{SPY,t-2} + \\beta_{SPY,3} r_{SPY,t-3} + \\beta_{IEF,1} r_{IEF,t-1} + \\beta_{IEF,2} r_{IEF,t-2} + \\beta_{IEF,3} r_{IEF,t-3}])]^{-1}$.\n",
    "Evaluate the performance of this model by printing the confusion matrix and accuracy on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "invisible-understanding",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[  0 146]\n",
      " [  0 156]]\n",
      "Accuracy: 0.5165562913907285\n",
      "F1: 0.6812227074235807\n",
      "roc_auc_score: 0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "# Split into training and testing\n",
    "data_length = len(df)\n",
    "train_size = int(data_length * 0.8)\n",
    "test_size = data_length - train_size\n",
    "\n",
    "train_data = df.iloc[:train_size]\n",
    "test_data = df.iloc[train_size:]\n",
    "\n",
    "# Define features and target variable for training and testing sets\n",
    "X_train = train_data[['SPY_lag1', 'SPY_lag2', 'SPY_lag3', 'IEF_lag1', 'IEF_lag2', 'IEF_lag3']]\n",
    "y_train = train_data['SPY>IEF']\n",
    "\n",
    "X_test = test_data[['SPY_lag1', 'SPY_lag2', 'SPY_lag3', 'IEF_lag1', 'IEF_lag2', 'IEF_lag3']]\n",
    "y_test = test_data['SPY>IEF']\n",
    "\n",
    "# Fit logistic regression model on scaled data\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on scaled test data\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc_score = roc_auc_score(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1:\", f1)\n",
    "print('roc_auc_score:', roc_auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual-alarm",
   "metadata": {},
   "source": [
    "<font color='red'>Solution:</font> it predicts just one class (1) with accuracy 51.65%. F1 score indicates the model has a good equilibrium between precision and recall. Roc Auc score says the model is not able to distinguish between positive and negative classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demographic-tiger",
   "metadata": {},
   "source": [
    "## Question 2 (20pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "major-lodging",
   "metadata": {},
   "source": [
    "### Question 2.1\n",
    "Using the same data, train/test split ratio, and consider the same classification problem as in Question 1.2.\n",
    "Create a (plain) recurrent neural network of your own design using a time step of 3.\n",
    "You may choose any activation functions you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fifteen-heart",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\fdimino\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\fdimino\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Construct neural network\n",
    "# Need to reshape for use in RNN (fa una sorte di expandign window)\n",
    "def get_XY(dat, time_steps):\n",
    "    # Indices of target array\n",
    "    Y_ind = np.arange(time_steps, len(dat), time_steps)\n",
    "    Y = dat['SPY>IEF'][Y_ind]\n",
    "    # Prepare X\n",
    "    rows_x = len(Y)\n",
    "    X = dat[['SPY_lag1', 'IEF_lag1']].iloc[range(time_steps*rows_x)]\n",
    "    X_array = X.values\n",
    "    X = np.reshape(X_array, (rows_x, time_steps, 2)) \n",
    "    return X, Y\n",
    " \n",
    "time_steps = 3 #similar to kernel size for TDNN.\n",
    "train_X, train_y = get_XY(train_data, time_steps)\n",
    "test_X, test_y = get_XY(test_data, time_steps)\n",
    "\n",
    "RNN = keras.Sequential()\n",
    "RNN.add(keras.layers.Input(shape=(3,2))) #shape = (time_steps,features)\n",
    "RNN.add(keras.layers.SimpleRNN(32, return_sequences=True, activation='relu'))\n",
    "# add return_sequences=True to all RNN layers except the last one to allow for stacking\n",
    "# Quando return_sequences=True è impostato, lo strato RNN restituisce un tensore per ogni passaggio temporale, il che significa che l'output sarà tridimensionale, con le dimensioni (batch_size, time_steps, output_features). Ciò consente di collegare direttamente uno strato RNN a un altro strato RNN successivo.\n",
    "\n",
    "RNN.add(keras.layers.SimpleRNN(32, activation='relu'))\n",
    "# Tuttavia, nell'ultimo strato RNN, di solito si imposta return_sequences=False (o si omette completamente) perché l'output dell'ultimo strato RNN deve essere un tensore bidimensionale, \n",
    "\n",
    "RNN.add(keras.layers.Dense(32, activation='relu')) # spetta a noi se metterlo o meno ma e` buono dato che vogliamo riportare al dense finale\n",
    "RNN.add(keras.layers.Dense(1, activation='sigmoid')) #1 output y\n",
    "\n",
    "# State the loss function and optimizer\n",
    "RNN.compile(optimizer=keras.optimizers.Adam(),loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "straight-breach",
   "metadata": {},
   "source": [
    "### Question 2.2\n",
    "Train this neural network on the training data.  \n",
    "Evaluate the performance of this model by printing the confusion matrix and accuracy on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "unlike-roads",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\fdimino\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\fdimino\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "Confusion Matrix:\n",
      "[[14 29]\n",
      " [21 36]]\n",
      "Accuracy: 0.5\n",
      "F1: 0.5901639344262295\n"
     ]
    }
   ],
   "source": [
    "# Fit the model to training data\n",
    "history = RNN.fit(train_X,train_y,epochs=100,validation_split=0.1,verbose=0)\n",
    "\n",
    "# Predictions on the test set\n",
    "y_pred_prob = RNN.predict(test_X)\n",
    "y_pred_binary = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "# Confusion matrix and accuracy\n",
    "conf_matrix = confusion_matrix(test_y, y_pred_binary)\n",
    "accuracy = accuracy_score(test_y, y_pred_binary)\n",
    "f1 = f1_score(test_y, y_pred_binary)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organic-martin",
   "metadata": {},
   "source": [
    "<font color='red'>Solution:</font> it predicts both classes but the accuracy and F1 score are slightly worse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdf4d0f",
   "metadata": {},
   "source": [
    "## Question 3 (20pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe5ad71",
   "metadata": {},
   "source": [
    "### Question 3.1\n",
    "Using the same data, train/test split ratio, and consider the same classification problem as in Question 1.2.\n",
    "Create a long short-term memory (LSTM) network of your own design using a time step of 3.\n",
    "You may choose any activation functions you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91b9beb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 3, 16)             1216      \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 16)                2112      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3345 (13.07 KB)\n",
      "Trainable params: 3345 (13.07 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "HIDDEN = 16\n",
    "LSTM = keras.Sequential()\n",
    "LSTM.add(keras.layers.Input(shape=(time_steps,2))) #shape = (time_steps,features)\n",
    "LSTM.add(keras.layers.LSTM(HIDDEN, return_sequences=True, activation='relu'))\n",
    "LSTM.add(keras.layers.LSTM(HIDDEN, activation='relu'))\n",
    "# qua non mettiamo dense dato che abbiamo gia molti parametri.\n",
    "LSTM.add(keras.layers.Dense(1, activation='sigmoid')) #1 output y\n",
    "\n",
    "# State the loss function and optimizer \n",
    "LSTM.compile(optimizer=keras.optimizers.Adam(),loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77177341",
   "metadata": {},
   "source": [
    "### Question 3.2\n",
    "Train this neural network on the training data.\n",
    "Evaluate the performance of this model by printing the confusion matrix and accuracy on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13e7422c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 4ms/step\n",
      "Confusion Matrix:\n",
      "[[ 0 43]\n",
      " [ 0 57]]\n",
      "Accuracy: 0.57\n",
      "F1: 0.7261146496815287\n"
     ]
    }
   ],
   "source": [
    "# Fit the model to training data\n",
    "history = LSTM.fit(train_X,train_y,epochs=100,validation_split=0.1,verbose=0)\n",
    "\n",
    "# Predictions on the test set\n",
    "y_pred_prob = LSTM.predict(test_X)\n",
    "y_pred_binary = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "# Confusion matrix and accuracy\n",
    "conf_matrix = confusion_matrix(test_y, y_pred_binary)\n",
    "accuracy = accuracy_score(test_y, y_pred_binary)\n",
    "f1 = f1_score(test_y, y_pred_binary)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be5e397",
   "metadata": {},
   "source": [
    "<font color='red'>Solution:</font> we have the best accuracy and f1 score, but it just predicts one class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "close-paradise",
   "metadata": {},
   "source": [
    "## Question 4 (20pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spread-olympus",
   "metadata": {},
   "source": [
    "## Question 4.1\n",
    "Consider the same classification problem as in Question 1.2.\n",
    "Of the methods considered in this assignment, which would you recommend in practice?\n",
    "Explain briefly (1 paragraph) why you choose this fit. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c9571c",
   "metadata": {},
   "source": [
    "<font color='red'>Solution:</font> I'd recommend the LSTM because it has the best accuracy and F1 score. However, one can argue that it just predicts one class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0bb2c9",
   "metadata": {},
   "source": [
    "## Question 4.2\n",
    "Recreate your data set using data from January 1, 2023 through December 31, 2023.\n",
    "Using the method your would implement in practice, invest in the asset (``SPY`` or ``IEF``) depending on your predictions.\n",
    "Print the returns your portfolio would obtain from following this strategy. Comment on how this portfolio compares with the ``SPY`` and ``IEF`` returns and risks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58c1f40a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  2 of 2 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_date = datetime(2023, 1, 1)\n",
    "end_date = datetime(2023, 12, 31)\n",
    "\n",
    "myData = yf.download(['SPY', 'IEF'], start=start_date, end=end_date)\n",
    "SPY = myData['Adj Close']['SPY']\n",
    "IEF = myData['Adj Close']['IEF']\n",
    "\n",
    "# Calculate daily log returns\n",
    "rSPY = (np.log(SPY) - np.log(SPY.shift(1))).dropna()\n",
    "rIEF = (np.log(IEF) - np.log(IEF.shift(1))).dropna()\n",
    "\n",
    "df = pd.DataFrame({'SPY': rSPY, 'IEF': rIEF})\n",
    "\n",
    "for i in range(1, 4):\n",
    "    df[f\"SPY_lag{i}\"] = df[\"SPY\"].shift(i)\n",
    "    df[f\"IEF_lag{i}\"] = df[\"IEF\"].shift(i)\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "df['SPY>IEF'] = (df['SPY'] > df['IEF']).astype(int)\n",
    "\n",
    "def get_XY(dat, time_steps):\n",
    "    # Indices of target array\n",
    "    Y_ind = np.arange(time_steps, len(dat), time_steps)\n",
    "    Y = dat['SPY>IEF'][Y_ind]\n",
    "    # Prepare X\n",
    "    rows_x = len(Y)\n",
    "    X = dat[['SPY_lag1', 'IEF_lag1']].iloc[range(time_steps*rows_x)]\n",
    "    X_array = X.values\n",
    "    X = np.reshape(X_array, (rows_x, time_steps, 2)) \n",
    "    return X, Y\n",
    " \n",
    "time_steps = 3 \n",
    "test_X, test_y = get_XY(df, time_steps)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_prob = LSTM.predict(test_X)\n",
    "y_pred_binary = (y_pred_prob > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f949e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rebalance\n",
    "y_pred_repeated = np.repeat(y_pred_binary[:-1], 3)\n",
    "y_pred_rebalanced = np.concatenate((y_pred_repeated, np.repeat(y_pred_binary[-1], 6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3811fee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Predictions'] = y_pred_rebalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "00c65e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Portfolio_Returns'] = np.where(df['Predictions'] == 1, df['SPY'], df['IEF'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f30241fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date\n",
      "2023-01-09   -0.000567\n",
      "2023-01-10    0.006988\n",
      "2023-01-11    0.012569\n",
      "2023-01-12    0.003634\n",
      "2023-01-13    0.003872\n",
      "                ...   \n",
      "2023-12-22    0.002008\n",
      "2023-12-26    0.004214\n",
      "2023-12-27    0.001806\n",
      "2023-12-28    0.000378\n",
      "2023-12-29   -0.002899\n",
      "Name: Portfolio_Returns, Length: 246, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df['Portfolio_Returns'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec03a69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0008855287850654267\n",
      "0.00812326478293696\n",
      "Spy mean : 0.0008855287850654267\n",
      "Spy sd: 0.00812326478293696\n",
      "Ief mean : 3.663575315225e-05\n",
      "Ief sd: 0.00586987043002503\n",
      "sharpe ratio portfolio: 0.10450146026217061\n"
     ]
    }
   ],
   "source": [
    "mean_p = df['Portfolio_Returns'].mean()\n",
    "sd_p = df['Portfolio_Returns'].std()\n",
    "\n",
    "print(mean_p)\n",
    "print(sd_p)\n",
    "\n",
    "# SPY\n",
    "mean_s = df['SPY'].mean()\n",
    "sd_s = df['SPY'].std()\n",
    "print('Spy mean :', mean_s)\n",
    "print('Spy sd:', sd_s)\n",
    "\n",
    "# IEF\n",
    "mean = df['IEF'].mean()\n",
    "sd = df['IEF'].std()\n",
    "print('Ief mean :', mean)\n",
    "print('Ief sd:', sd)\n",
    "\n",
    "# portfolio\n",
    "sharpe_ratio = (mean_p-mean)/sd_p\n",
    "print('sharpe ratio portfolio:', sharpe_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470a0145",
   "metadata": {},
   "source": [
    "<font color='red'>Solution:</font> since we just predict SPY, the portfolio returns and volatility are the same as SPY. SD is slightly higher than IEF, but we have better returns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
