{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aware-spouse",
   "metadata": {},
   "source": [
    "Name: Lorenzo Ausiello\n",
    "\n",
    "Date: 02/05/2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fundamental-defense",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import yfinance\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Set seed of random number generator\n",
    "CWID = 20021869 \n",
    "personal = CWID % 10000\n",
    "np.random.seed(personal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handed-rebound",
   "metadata": {},
   "source": [
    "## Question 1 (10pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pediatric-shoulder",
   "metadata": {},
   "source": [
    "### Question 1.1 \n",
    "Use the `yfinance` package (or other method of your choice) to obtain the daily adjusted close prices for `SPY` and `IEF`.  You should have at least 5 years of data for both assets. Do **not** include any data after January 1, 2023.  You should inspect the dates for your data to make sure you are including everything appropriately.  Create a binary variable whether the `SPY` returns are above the `IEF` returns on a each day. Create a data frame (or array) of the daily log returns both both stocks along with the lagged returns (at least 3 lags) and your binary class variable.  Use the `print` command to display your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ae435c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  2 of 2 completed\n",
      "                 SPY       IEF  SPY_lag1  IEF_lag1  SPY_lag2  IEF_lag2  \\\n",
      "Date                                                                     \n",
      "2017-01-09 -0.003306  0.003799  0.003571 -0.004558 -0.000795  0.006463   \n",
      "2017-01-10  0.000000 -0.000474 -0.003306  0.003799  0.003571 -0.004558   \n",
      "2017-01-11  0.002822  0.001137  0.000000 -0.000474 -0.003306  0.003799   \n",
      "2017-01-12 -0.002513  0.000569  0.002822  0.001137  0.000000 -0.000474   \n",
      "2017-01-13  0.002293 -0.002180 -0.002513  0.000569  0.002822  0.001137   \n",
      "...              ...       ...       ...       ...       ...       ...   \n",
      "2022-12-23  0.005736 -0.004537 -0.014369 -0.000309  0.014842  0.001235   \n",
      "2022-12-27 -0.003951 -0.008407  0.005736 -0.004537 -0.014369 -0.000309   \n",
      "2022-12-28 -0.012506 -0.002400 -0.003951 -0.008407  0.005736 -0.004537   \n",
      "2022-12-29  0.017840  0.004898 -0.012506 -0.002400 -0.003951 -0.008407   \n",
      "2022-12-30 -0.002638 -0.004167  0.017840  0.004898 -0.012506 -0.002400   \n",
      "\n",
      "            SPY_lag3  IEF_lag3  SPY>IEF  \n",
      "Date                                     \n",
      "2017-01-09  0.005932  0.001144        0  \n",
      "2017-01-10 -0.000795  0.006463        1  \n",
      "2017-01-11  0.003571 -0.004558        1  \n",
      "2017-01-12 -0.003306  0.003799        0  \n",
      "2017-01-13  0.000000 -0.000474        1  \n",
      "...              ...       ...      ...  \n",
      "2022-12-23  0.001368 -0.007285        1  \n",
      "2022-12-27  0.014842  0.001235        1  \n",
      "2022-12-28 -0.014369 -0.000309        0  \n",
      "2022-12-29  0.005736 -0.004537        1  \n",
      "2022-12-30 -0.003951 -0.008407        1  \n",
      "\n",
      "[1506 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Download historical data for SPY and IEF\n",
    "start_date = datetime(2017, 1, 1)\n",
    "end_date = datetime(2023, 1, 1)\n",
    "\n",
    "myData = yf.download(['SPY', 'IEF'], start=start_date, end=end_date)\n",
    "SPY = myData['Adj Close']['SPY']\n",
    "IEF = myData['Adj Close']['IEF']\n",
    "\n",
    "# Calculate daily log returns\n",
    "rSPY = (np.log(SPY) - np.log(SPY.shift(1))).dropna()\n",
    "rIEF = (np.log(IEF) - np.log(IEF.shift(1))).dropna()\n",
    "\n",
    "df = pd.DataFrame({'SPY':rSPY, 'IEF': rIEF})\n",
    "\n",
    "for i in range(1, 4):\n",
    "    df[f\"SPY_lag{i}\"] = df[\"SPY\"].shift(i)\n",
    "    df[f\"IEF_lag{i}\"] = df[\"IEF\"].shift(i)\n",
    "    \n",
    "df= df.dropna()\n",
    "\n",
    "df['SPY>IEF'] = (df['SPY'] > df['IEF']).astype(int)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "progressive-range",
   "metadata": {},
   "source": [
    "### Question 1.2\n",
    "Split your data into training and testing sets (80% training and 20% test). This split should be done so that the causal relationship is kept consistent (i.e., split data at a specific time).\n",
    "\n",
    "Run a logistic regression of the binary variable (of `SPY` returns greater than `IEF` returns) as a function of the lagged returns (at least 2 lags) for both stocks.\n",
    "This should be of the form (assuming 2 lags) of $p_{t} = [1 + \\exp(-[\\beta_0 + \\beta_{SPY,1} r_{SPY,t-1} + \\beta_{SPY,2} r_{SPY,t-2} + \\beta_{SPY,3} r_{SPY,t-3} + \\beta_{IEF,1} r_{IEF,t-1} + \\beta_{IEF,2} r_{IEF,t-2} + \\beta_{IEF,3} r_{IEF,t-3}])]^{-1}$.\n",
    "Evaluate the performance of this model by printing the confusion matrix and accuracy on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "invisible-understanding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[  0 146]\n",
      " [  0 156]]\n",
      "Accuracy: 0.5165562913907285\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Get the length of the DataFrame\n",
    "data_length = len(df)\n",
    "\n",
    "# Calculate the number of samples for each set\n",
    "train_size = int(data_length * 0.8)\n",
    "test_size = data_length - train_size\n",
    "\n",
    "# Split into training and testing sets based on index\n",
    "train_data = df.iloc[:train_size]\n",
    "test_data = df.iloc[train_size:]\n",
    "\n",
    "# Define features and target variable for training and testing sets\n",
    "X_train = train_data[['SPY_lag1', 'SPY_lag2', 'SPY_lag3', 'IEF_lag1', 'IEF_lag2', 'IEF_lag3']]\n",
    "y_train = train_data['SPY>IEF']\n",
    "\n",
    "X_test = test_data[['SPY_lag1', 'SPY_lag2', 'SPY_lag3', 'IEF_lag1', 'IEF_lag2', 'IEF_lag3']]\n",
    "y_test = test_data['SPY>IEF']\n",
    "\n",
    "# Fit logistic regression model on scaled data\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on scaled test data\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual-alarm",
   "metadata": {},
   "source": [
    "<font color='red'>Solution:</font> The accuracy is 48%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demographic-tiger",
   "metadata": {},
   "source": [
    "## Question 2 (30spt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "major-lodging",
   "metadata": {},
   "source": [
    "### Question 2.1\n",
    "Using the same data, train/test split ratio, and consider the same classification problem as in Question 1.2.\n",
    "Create a feed-forward neural network with a single hidden layer (8 hidden nodes) densely connected to the inputs.\n",
    "You may choose any activation functions you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fifteen-heart",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\loaus\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\loaus\\anaconda3\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 8)                 56        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 65 (260.00 Byte)\n",
      "Trainable params: 65 (260.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Hidden layer\n",
    "model.add(keras.layers.Dense(units=8, input_dim=6, activation= 'relu'))\n",
    "\n",
    "# Output layer\n",
    "model.add(keras.layers.Dense(1, activation = \"sigmoid\"))\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "straight-breach",
   "metadata": {},
   "source": [
    "### Question 2.2\n",
    "Train this neural network on the training data.  \n",
    "Evaluate the performance of this model by printing the confusion matrix and accuracy on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c227f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\loaus\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\loaus\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "Test Loss: 0.7014026045799255\n",
      "Test Accuracy: 0.4867549538612366\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "[[ 20 126]\n",
      " [ 29 127]]\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=500, verbose = 0)\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred_binary = (y_pred_prob > 0.5).astype(int)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_binary)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organic-martin",
   "metadata": {},
   "source": [
    "<font color='red'>Solution:</font> we have a similar accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standard-methodology",
   "metadata": {},
   "source": [
    "### Question 2.3\n",
    "Using the same train/test split and consider the same classification problem as in Question 1.2.\n",
    "Train and test another feed-forward neural network of your own design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "collective-briefs",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\loaus\\anaconda3\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "10/10 [==============================] - 0s 3ms/step\n",
      "[[ 32 114]\n",
      " [ 35 121]]\n",
      "0.5066225165562914\n"
     ]
    }
   ],
   "source": [
    "# Hidden layer\n",
    "model.add(keras.layers.Dense(units=32, input_dim=6, activation= 'relu'))\n",
    "\n",
    "# Add another hidden layer with 8 units and ReLU activation\n",
    "model.add(keras.layers.Dense(units=16, activation='relu'))\n",
    "\n",
    "# Output layer with 1 unit\n",
    "model.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train, epochs=500, verbose = 0)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred_binary = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "# Confusion matrix and accuracy\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_binary)\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "\n",
    "print(conf_matrix)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changed-suicide",
   "metadata": {},
   "source": [
    "<font color='red'>Solution:</font> in this case, we have less accuracy than the previous one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdf4d0f",
   "metadata": {},
   "source": [
    "## Question 3 (30pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe5ad71",
   "metadata": {},
   "source": [
    "### Question 3.1\n",
    "Using the same data, train/test split ratio, and consider the same classification problem as in Question 1.2.\n",
    "Create a time dilation neural network with a single convolutional layer (filter size of 8, kernel size of 3, dilation size of 1) densely connected to the inputs.\n",
    "You may choose any activation functions you wish.\n",
    "\n",
    "*Hint:* The CNN can reference earlier lags on its own without feeding explicit memory inputs as was needed for the Question 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "91b9beb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_1 (Conv1D)           (None, 2, 8)              32        \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 49 (196.00 Byte)\n",
      "Trainable params: 49 (196.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Conv1D, Dense, Flatten\n",
    "TDNN = keras.Sequential()\n",
    "\n",
    "TDNN.add(keras.layers.Conv1D(filters=8, kernel_size=3, padding='causal', dilation_rate=1, activation='relu', input_shape=(2, 1)))\n",
    "TDNN.add(keras.layers.Flatten())\n",
    "TDNN.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "TDNN.compile(optimizer=keras.optimizers.Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "X_train1 = train_data[['SPY_lag1', 'IEF_lag1']].values\n",
    "X_test1 = test_data[['SPY_lag1', 'IEF_lag1']].values\n",
    "\n",
    "# Reshape data for Conv1D input\n",
    "X_train_reshaped = X_train1.reshape((X_train1.shape[0], X_train1.shape[1], 1))\n",
    "X_test_reshaped = X_test1.reshape((X_test1.shape[0], X_test1.shape[1], 1))\n",
    "TDNN.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77177341",
   "metadata": {},
   "source": [
    "### Question 3.2\n",
    "Train this neural network on the training data.\n",
    "Evaluate the performance of this model by printing the confusion matrix and accuracy on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "13e7422c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6981911063194275\n",
      "Test Accuracy: 0.5298013091087341\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "[[  9 137]\n",
      " [  5 151]]\n",
      "0.5298013245033113\n"
     ]
    }
   ],
   "source": [
    "TDNN.fit(X_train_reshaped, y_train, epochs=500, verbose=0)\n",
    "\n",
    "test_loss, test_accuracy = TDNN.evaluate(X_test_reshaped, y_test, verbose=0)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "# Predictions on the test set\n",
    "y_pred_prob = TDNN.predict(X_test_reshaped)\n",
    "y_pred_binary = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "# Confusion matrix and accuracy\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_binary)\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "\n",
    "print(conf_matrix)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be5e397",
   "metadata": {},
   "source": [
    "<font color='red'>Solution:</font> in this case we have a similar accuracy compared to the previous models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623518bb",
   "metadata": {},
   "source": [
    "### Question 3.3\n",
    "Using the same train/test split and consider the same classification problem as in Question 1.2. Train and test another convolutional neural network of your own design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "965e78bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_6 (Conv1D)           (None, 2, 32)             128       \n",
      "                                                                 \n",
      " conv1d_7 (Conv1D)           (None, 2, 16)             1552      \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1713 (6.69 KB)\n",
      "Trainable params: 1713 (6.69 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Test Loss: 0.7020733952522278\n",
      "Test Accuracy: 0.49668875336647034\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "[[ 12 134]\n",
      " [ 18 138]]\n",
      "0.4966887417218543\n"
     ]
    }
   ],
   "source": [
    "TDNN1 = keras.Sequential()\n",
    "\n",
    "TDNN1.add(keras.layers.Conv1D(filters=32, kernel_size=3, padding='causal', dilation_rate=1, activation='relu', input_shape=(2, 1)))\n",
    "TDNN1.add(keras.layers.Conv1D(filters=16, kernel_size=3, padding='causal', dilation_rate=1, activation='relu', input_shape=(2, 1)))\n",
    "TDNN1.add(keras.layers.Flatten())\n",
    "TDNN1.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "TDNN1.compile(optimizer=keras.optimizers.Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "TDNN1.summary()\n",
    "         \n",
    "TDNN1.fit(X_train_reshaped, y_train, epochs=500, verbose=0)\n",
    "\n",
    "test_loss, test_accuracy = TDNN1.evaluate(X_test_reshaped, y_test, verbose=0)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "# Predictions on the test set\n",
    "y_pred_prob = TDNN1.predict(X_test_reshaped)\n",
    "y_pred_binary = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "# Confusion matrix and accuracy\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_binary)\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "\n",
    "print(conf_matrix)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4216515",
   "metadata": {},
   "source": [
    "<font color='red'>Solution:</font> we have less accuracy than previous one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "close-paradise",
   "metadata": {},
   "source": [
    "## Question 4 (30pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spread-olympus",
   "metadata": {},
   "source": [
    "## Question 4.1\n",
    "Consider the same classification problem as in Question 1.2.\n",
    "Of the methods considered in this assignment, which would you recommend in practice?\n",
    "Explain briefly (1 paragraph) why you choose this fit. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c9571c",
   "metadata": {},
   "source": [
    "<font color='red'>Solution:</font> the method with a good accuracy is the TDNN (48%). I use this method because Iwe have less accuracy compared to logistic regression, but it predicts also class 0 e not just class 1. I reccomend this method but also other methods have a similar accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0bb2c9",
   "metadata": {},
   "source": [
    "## Question 4.2\n",
    "Recreate your data set using data from January 1, 2023 through December 31, 2023.\n",
    "Using the method your would implement in practice, invest in the asset (``SPY`` or ``IEF``) depending on your predictions.\n",
    "Print the returns your portfolio would obtain from following this strategy. Comment on how this portfolio compares with the ``SPY`` and ``IEF`` returns and risks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "35ec004d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  2 of 2 completed\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "                 SPY       IEF  SPY_lag1  IEF_lag1  SPY_lag2  IEF_lag2  \\\n",
      "Date                                                                     \n",
      "2023-01-09 -0.000567  0.002538  0.022673  0.012787 -0.011479 -0.001440   \n",
      "2023-01-10  0.006988 -0.006306 -0.000567  0.002538  0.022673  0.012787   \n",
      "2023-01-11  0.012569  0.006407  0.006988 -0.006306 -0.000567  0.002538   \n",
      "2023-01-12  0.003634  0.008882  0.012569  0.006407  0.006988 -0.006306   \n",
      "2023-01-13  0.003872 -0.005340  0.003634  0.008882  0.012569  0.006407   \n",
      "...              ...       ...       ...       ...       ...       ...   \n",
      "2023-12-22  0.002008 -0.000935  0.009437 -0.001557 -0.013954  0.004886   \n",
      "2023-12-26  0.004214  0.000624  0.002008 -0.000935  0.009437 -0.001557   \n",
      "2023-12-27  0.001806  0.007762  0.004214  0.000624  0.002008 -0.000935   \n",
      "2023-12-28  0.000378 -0.003822  0.001806  0.007762  0.004214  0.000624   \n",
      "2023-12-29 -0.002899 -0.002487  0.000378 -0.003822  0.001806  0.007762   \n",
      "\n",
      "            SPY_lag3  IEF_lag3  SPY>IEF  Predictions  Portfolio_Returns  \n",
      "Date                                                                     \n",
      "2023-01-09  0.007691  0.007637        0            1          -0.000567  \n",
      "2023-01-10 -0.011479 -0.001440        1            1           0.006988  \n",
      "2023-01-11  0.022673  0.012787        1            1           0.012569  \n",
      "2023-01-12 -0.000567  0.002538        0            1           0.003634  \n",
      "2023-01-13  0.006988 -0.006306        1            1           0.003872  \n",
      "...              ...       ...      ...          ...                ...  \n",
      "2023-12-22  0.006062  0.001878        1            1           0.002008  \n",
      "2023-12-26 -0.013954  0.004886        1            1           0.004214  \n",
      "2023-12-27  0.009437 -0.001557        0            1           0.001806  \n",
      "2023-12-28  0.002008 -0.000935        1            1           0.000378  \n",
      "2023-12-29  0.004214  0.000624        0            1          -0.002899  \n",
      "\n",
      "[246 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "start_date = datetime(2023, 1, 1)\n",
    "end_date = datetime(2023, 12, 31)\n",
    "\n",
    "myData = yf.download(['SPY', 'IEF'], start=start_date, end=end_date)\n",
    "SPY = myData['Adj Close']['SPY']\n",
    "IEF = myData['Adj Close']['IEF']\n",
    "\n",
    "# Calculate daily log returns\n",
    "rSPY = (np.log(SPY) - np.log(SPY.shift(1))).dropna()\n",
    "rIEF = (np.log(IEF) - np.log(IEF.shift(1))).dropna()\n",
    "\n",
    "df = pd.DataFrame({'SPY': rSPY, 'IEF': rIEF})\n",
    "\n",
    "for i in range(1, 4):\n",
    "    df[f\"SPY_lag{i}\"] = df[\"SPY\"].shift(i)\n",
    "    df[f\"IEF_lag{i}\"] = df[\"IEF\"].shift(i)\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "df['SPY>IEF'] = (df['SPY'] > df['IEF']).astype(int)\n",
    "\n",
    "# Define features and target variable for training and testing sets\n",
    "X_predict = df[['SPY_lag1', 'IEF_lag1']].values\n",
    "y = df['SPY>IEF']\n",
    "\n",
    "X_predict_reshaped = X_predict.reshape((X_predict.shape[0], X_predict.shape[1], 1))\n",
    "\n",
    "# Make predictions, notice \n",
    "y_pred_prob = TDNN.predict(X_predict_reshaped)\n",
    "y_pred_binary = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "# Calculate portfolio returns\n",
    "df['Predictions'] = y_pred_binary\n",
    "\n",
    "df['Portfolio_Returns'] = np.where(df['Predictions'] == 1, df['SPY'], df['IEF'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "df6a93d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27785877165972805\n",
      "0.12823682216769253\n",
      "Spy mean : 0.22315317205683186\n",
      "Spy sd: 0.1289527803460459\n",
      "Ief mean : 0.009232127777839577\n",
      "Ief sd: 0.09318130785619633\n",
      "sharpe ratio portfolio: 2.094769968103321\n"
     ]
    }
   ],
   "source": [
    "mean_p = df['Portfolio_Returns'].mean()*252\n",
    "sd_p = df['Portfolio_Returns'].std()*252**0.5\n",
    "\n",
    "print(mean_p)\n",
    "print(sd_p)\n",
    "\n",
    "# SPY\n",
    "mean_s = df['SPY'].mean()*252\n",
    "sd_s = df['SPY'].std()*252**0.5\n",
    "print('Spy mean :', mean_s)\n",
    "print('Spy sd:', sd_s)\n",
    "\n",
    "# IEF\n",
    "mean = df['IEF'].mean()*252\n",
    "sd = df['IEF'].std()*252**0.5\n",
    "print('Ief mean :', mean)\n",
    "print('Ief sd:', sd)\n",
    "\n",
    "# portfolio\n",
    "sharpe_ratio = (mean_p-mean)/sd_p\n",
    "print('sharpe ratio portfolio:', sharpe_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470a0145",
   "metadata": {},
   "source": [
    "<font color='red'>Solution:</font> this method allows us to invest mainly in SPY but we have a little better expected returns, sd and sharpe ratio than SPY."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
